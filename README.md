# GPT-4o Code Runtime Evaluation

This project is designed to evaluate the performance of code generated by GPT-4o in solving various computer science problems. The project runs tests on generated code, measures runtime performance, and outputs detailed and summarized results.

## Project Structure

The project consists of several core components:

1. **ChatGPTPrompt**: Handles prompt-related operations such as retrieving the file path for generated code and extracting code segments.
2. **RuntimeExecution**: Executes dynamically defined code and measures its runtime performance.
3. **RuntimePerformance**: Orchestrates the execution of generated code, using multiprocessing to measure minimum, average, and maximum runtimes.
4. **WriteRuntimeSummaryDetailedInformation**: Writes detailed and summary runtime performance information to CSV files and calculates percentage differences.
5. **ProblemSet**: Initializes and handles problem-specific code, mapping problem numbers to their respective implementations.

## Classes Overview

### 1. ChatGPTPrompt
- **Purpose**: Handles reading the generated code from CSV files, extracting code segments, and validating code correctness.
- **Key Methods**:
  - `get_file_path(directory_path)`: Retrieves file paths based on the given directory.
  - `get_code_segment(generated_code)`: Extracts the Python code from a Markdown-formatted string.

### 2. RuntimeExecution
- **Purpose**: Manages the execution of dynamically defined code, measuring the runtime of each version.
- **Key Methods**:
  - `execute(size, *args)`: Executes the dynamically generated code and measures the runtime using `timeit`.

### 3. RuntimePerformance
- **Purpose**: Executes the GPT-4o generated code for different input sizes, records runtime data, and handles multiprocessing to execute code asynchronously.
- **Key Methods**:
  - `get_runtime(prompt_name, code_index, func_code)`: Runs the code and measures the minimum, average, and maximum runtime for each test case.

### 4. WriteRuntimeSummaryDetailedInformation
- **Purpose**: Writes runtime performance information to CSV files, including both detailed and summary statistics, and calculates percentage differences.
- **Key Methods**:
  - `write_runtime_detailed_information(data, output_detailed_file)`: Writes detailed runtime information to a CSV file.
  - `write_runtime_summary_information(output_detailed_file, output_summary_file)`: Writes summary runtime information based on detailed runtime data.
  - `calc_percentage(output_summary_file)`: Calculates and writes percentage differences between average runtimes and the minimum average.

### 5. ProblemSet
- **Purpose**: Handles problem-specific initialization and execution based on the problem number.
- **Key Methods**:
  - `create_problem_instance()`: Maps problem numbers to the respective problem class.
  - `handle_problem_number()`: Returns information about the selected problem.

## Project Workflow

1. **Prompt Retrieval**: The code first retrieves the necessary prompt files and code segments using the `ChatGPTPrompt` class.
2. **Code Execution**: The `RuntimePerformance` class is used to execute the code for various input sizes. It leverages multiprocessing to ensure that the code runs asynchronously, preventing timeouts.
3. **Runtime Measurement**: Using `RuntimeExecution`, the code is executed, and the runtime (min, avg, and max) is recorded for each input size.
4. **Writing Results**: The `WriteRuntimeSummaryDetailedInformation` class writes the detailed and summary runtime information to CSV files, then calculates percentage differences in runtimes.

## Installation

1. Clone this repository:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Ensure the project directory structure follows this format:
   ```
   ├── code_generator
   │   └── data
   ├── results
   ├── runtime_performance.py
   ├── chatgpt_prompt.py
   ├── runtime_execution.py
   ├── write_runtime_summary_detailed_information.py
   ├── problem_set.py
   └── main.py
   ```

## Running the Evaluation

To run the evaluation for a specific problem:

1. Define the problem number, sizes, and model in the `main.py` script.
2. Execute the script:
   ```bash
   python main.py
   ```

The script will run the GPT-4o-generated code, measure its runtime performance, and store the results in the `results` folder.

## Output

1. **Detailed Runtime File**: This file contains detailed runtime data for each test case, including the min, avg, and max times for each input size.
2. **Summary Runtime File**: This file summarizes the runtime data and calculates the percentage difference from the minimum average runtime for each input size.

## Example

```bash
python main.py
```

Example output:
- `p1_gpt_4o_detailed_auto_execution_times.csv`
- `p1_gpt_4o_summary_auto_execution_times.csv`

These files contain the detailed and summary performance metrics for problem 1.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
