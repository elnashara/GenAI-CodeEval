# GPT-4o Code Runtime Evaluation (Experiment 1)

This project is designed to evaluate the performance of code generated by GPT-4o in solving various computer science problems. The project runs tests on generated code, measures runtime performance, and outputs detailed and summarized results.

## Project Structure (Experiment 1)

The project consists of several core components:

1. **ChatGPTPrompt**: Handles prompt-related operations such as retrieving the file path for generated code and extracting code segments.
2. **RuntimeExecution**: Executes dynamically defined code and measures its runtime performance.
3. **RuntimePerformance**: Orchestrates the execution of generated code, using multiprocessing to measure minimum, average, and maximum runtimes.
4. **WriteRuntimeSummaryDetailedInformation**: Writes detailed and summary runtime performance information to CSV files and calculates percentage differences.
5. **ProblemSet**: Initializes and handles problem-specific code, mapping problem numbers to their respective implementations.

## Classes Overview (Experiment 1)

### 1. ChatGPTPrompt
- **Purpose**: Handles reading the generated code from CSV files, extracting code segments, and validating code correctness.
- **Key Methods**:
  - `get_file_path(directory_path)`: Retrieves file paths based on the given directory.
  - `get_code_segment(generated_code)`: Extracts the Python code from a Markdown-formatted string.

### 2. RuntimeExecution
- **Purpose**: Manages the execution of dynamically defined code, measuring the runtime of each version.
- **Key Methods**:
  - `execute(size, *args)`: Executes the dynamically generated code and measures the runtime using `timeit`.

### 3. RuntimePerformance
- **Purpose**: Executes the GPT-4o generated code for different input sizes, records runtime data, and handles multiprocessing to execute code asynchronously.
- **Key Methods**:
  - `get_runtime(prompt_name, code_index, func_code)`: Runs the code and measures the minimum, average, and maximum runtime for each test case.

### 4. WriteRuntimeSummaryDetailedInformation
- **Purpose**: Writes runtime performance information to CSV files, including both detailed and summary statistics, and calculates percentage differences.
- **Key Methods**:
  - `write_runtime_detailed_information(data, output_detailed_file)`: Writes detailed runtime information to a CSV file.
  - `write_runtime_summary_information(output_detailed_file, output_summary_file)`: Writes summary runtime information based on detailed runtime data.
  - `calc_percentage(output_summary_file)`: Calculates and writes percentage differences between average runtimes and the minimum average.

### 5. ProblemSet
- **Purpose**: Handles problem-specific initialization and execution based on the problem number.
- **Key Methods**:
  - `create_problem_instance()`: Maps problem numbers to the respective problem class.
  - `handle_problem_number()`: Returns information about the selected problem.

## Project Workflow (Experiment 1)

1. **Prompt Retrieval**: The code first retrieves the necessary prompt files and code segments using the `ChatGPTPrompt` class.
2. **Code Execution**: The `RuntimePerformance` class is used to execute the code for various input sizes. It leverages multiprocessing to ensure that the code runs asynchronously, preventing timeouts.
3. **Runtime Measurement**: Using `RuntimeExecution`, the code is executed, and the runtime (min, avg, and max) is recorded for each input size.
4. **Writing Results**: The `WriteRuntimeSummaryDetailedInformation` class writes the detailed and summary runtime information to CSV files, then calculates percentage differences in runtimes.

## Installation (Experiment 1)

1. Clone this repository:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Ensure the project directory structure follows this format:
   ```
   ├── code_generator
   │   └── data
   ├── results
   ├── runtime_performance.py
   ├── chatgpt_prompt.py
   ├── runtime_execution.py
   ├── write_runtime_summary_detailed_information.py
   ├── problem_set.py
   └── main.py
   ```

## Running the Evaluation (Experiment 1)

To run the evaluation for a specific problem:

1. Define the problem number, sizes, and model in the `main.py` script.
2. Execute the script:
   ```bash
   python main.py
   ```

The script will run the GPT-4o-generated code, measure its runtime performance, and store the results in the `results` folder.

## Output (Experiment 1)

1. **Detailed Runtime File**: This file contains detailed runtime data for each test case, including the min, avg, and max times for each input size.
2. **Summary Runtime File**: This file summarizes the runtime data and calculates the percentage difference from the minimum average runtime for each input size.

## Example (Experiment 1)

```bash
python main.py
```

Example output:
- `p1_gpt_4o_detailed_auto_execution_times.csv`
- `p1_gpt_4o_summary_auto_execution_times.csv`

These files contain the detailed and summary performance metrics for problem 1.

---

# GPT-4o and AutoGen Code Evaluation (Experiment 2)

This project is designed to evaluate the performance of code generated by **GPT-4o** and **AutoGen** in solving computer science problems. The experiment runs tests on generated code, measures runtime performance, and logs test results in CSV format.

## Project Structure (Experiment 2)

This project is split into two primary experiments:
1. **GPT-4o Task Manager**: Generates and tests code using GPT-4o.
2. **AutoGen Task Manager**: Automates code generation and testing using AutoGen’s Assistant and User Proxy agents.

## Experiments Overview (Experiment 2)

### 1. GPT-4o Task Manager
- **Class**: `GptTaskManager`
- **Purpose**: Manages interaction with the GPT-4o API, generates solutions, and logs test results.
- **Key Features**:
  - Generate solution prompts for GPT-4o to solve various computer science problems.
  - Execute the generated Python code to validate solutions.
  - Log test results, including input, expected output, actual output, and exception handling, to a CSV file.
  
#### Key Methods:
- **`call_gpt_chat_api(prompt)`**: Calls the GPT-4o API with the provided prompt and returns the response.
- **`generate_solution_prompt(problem)`**: Generates a solution prompt for GPT-4o to develop a Python script for solving a problem.
- **`generate_solution(problem)`**: Uses GPT-4o to generate a solution for a given problem.
- **`log_test_results(solution_python, file_name)`**: Logs the test results to a CSV file after running the solution.

### 2. AutoGen Task Manager
- **Class**: `AutoGenTaskManager`
- **Purpose**: Automates solution generation and testing using AutoGen's Assistant and User Proxy agents.
- **Key Features**:
  - Automates the interaction between the User Proxy and Assistant agents.
  - Generates Python code to solve computer science problems.
  - Automates the logging of test results and output in a structured CSV format.
  
#### Key Methods:
- **`initiate_solution_generation(SolutionGenerationPrompt)`**: Initiates the solution generation process through the User Proxy agent.
- **`initiate_test_logging(test_result_logging_prompt)`**: Automates logging of test results via the User Proxy agent.
- **`capture_output(solution_prompt)`**: Captures the output from the solution generation process.
- **`extract_last_python_code(text)`**: Extracts the last Python code block generated in the conversation.

## Setup and Installation (Experiment 2)

### Prerequisites
- Python 3.x
- `pandas` library
- API keys for OpenAI's GPT-4o

### Installation

1. Clone this repository:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Create a file named `api_key` in the root directory and store your OpenAI API key inside:
   ```
   <your_openai_api_key>
   ```

## Running the Experiments (Experiment 2)

### 1. Running the GPT-4o Task Manager

To run the GPT-4o code evaluation:

1. Prepare a CSV file (`computer_science_problems.csv`) with your problem descriptions.
2. Modify the parameters in the `main.py` script:
   - Set the problem number and parameters.
   - Update the paths for the input and output files.
   
3. Run the script:
   ```bash
   python main.py
   ```

### 2. Running the AutoGen Task Manager

To run the AutoGen task evaluation:

1. Ensure the problem descriptions are loaded from the CSV file (`computer_science_problems.csv`).
2. Modify the `AutoGenTaskManager` to match the desired configurations.
3. Run the script:
   ```bash
   python autogen_main.py
   ```

## Output (Experiment 2)

The results of the evaluations will be logged in CSV files in the `results` directory. Each file will contain:
- Problem descriptions
- Generated code solutions
- Input and output for each test case
- Runtime performance statistics

### Example output files (Experiment 2):
- `1_Prompt1_ChatGPT4o_NaiveApproach.csv`
- `1_Prompt1_AutoGen_NaiveApproach.csv`

These files will contain the detailed and summary performance metrics for each problem solved using both **GPT-4o** and **AutoGen**.

## CSV Output Structure (Experiment 2)

Each CSV file contains the following columns:
- **Problem Number**: The ID of the problem.
- **Category**: The problem category.
- **Problem Type**: The type of the problem (e.g., algorithm, data structure).
- **Problem**: The problem description.
- **Solution Prompt**: The prompt used to generate the solution.
- **Test Input**: The input data for the test case.
- **Expected Output**: The expected output of the test case.
- **Actual Output**: The actual output from running the solution.
- **Status**: Indicates if the test case passed or failed.
- **Pass**: Boolean value (True if passed, False otherwise).
- **Exception**: Any exceptions encountered during execution.

### Example CSV File Entry (Experiment 2):

```csv
problem_number, index, category, problem_type, problem, solution_prompt, solution, test_input, expected, actual, status, pass, exception
1, 0, Sorting, Bubble Sort, Sort an array of integers, ... , ..., [1, 3, 2, 4], [1, 2, 3, 4], PASS, True, None
```

## License

This project is licensed under the Vanderbilt License. See the [LICENSE](LICENSE) file for details.
